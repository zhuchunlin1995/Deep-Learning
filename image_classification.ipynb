{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhuchunlin1995/Deep-Learning/blob/master/image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6SD-AONjrguR",
        "colab_type": "code",
        "outputId": "dd4c14b6-59ff-4b5f-adc7-4847ce81e632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#download the CIFAR10 dataset for image classification.\n",
        "import torch, torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "transform = torchvision.transforms.Compose( [torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "\n",
        "#preprocess trainset.\n",
        "dataset = []\n",
        "#flat dataset\n",
        "for data in trainset:\n",
        "  tmp = []\n",
        "  tmp.append(data[0].numpy().ravel())\n",
        "  tmp.append(data[1])\n",
        "  dataset.append(tmp)\n",
        "\n",
        "\n",
        "random.shuffle(dataset)\n",
        "#separate features and label\n",
        "dataset_X = [item[0].tolist() for item in dataset]\n",
        "dataset_Y = [item[1] for item in dataset]\n",
        "\n",
        "#combined training sample:\n",
        "training_set = dataset[5000:]\n",
        "\n",
        "#split dataset into training set and validation set and format the dataset\n",
        "\n",
        "valiset_X = np.asarray(dataset_X[:5000])\n",
        "valiset_Y = dataset_Y[:5000]\n",
        "\n",
        "\n",
        "trainset_X = np.asarray(dataset_X[5000:]) #shape (num of training data, num of features)\n",
        "trainset_Y = dataset_Y[5000:]\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EtTKFOi5g1IJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1292
        },
        "outputId": "46dfbde1-3a15-424b-d35f-94c20a0a450b"
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  \n",
        "  def __init__(self, layer_dimensions):\n",
        "    #initialize weights and bias\n",
        "    self.parameters = {}\n",
        "    self.cache = {}\n",
        "    self.num_layers = len(layer_dimensions) - 1\n",
        "    for l in range (1, self.num_layers + 1):\n",
        "      eps = np.sqrt(2.0 / (layer_dimensions[l] + layer_dimensions[l - 1]))\n",
        "      self.parameters[\"W\" + str(l)] = np.random.randn(layer_dimensions[l], layer_dimensions[l - 1]) * eps #shape (features, neurons)\n",
        "      self.parameters[\"b\" + str(l)] = np.zeros((layer_dimensions[l], 1)) + 0.01 #shape (1,n)\n",
        "  \n",
        "  def affineForward(self, A, W, b, layer):\n",
        "    forward = np.dot(W, A) + b\n",
        "    self.cache[\"Z\" + str(layer)] = forward\n",
        "    return forward\n",
        "    \n",
        "  def activationForward(self, A, layer):\n",
        "    # use relu function as activation function\n",
        "    A = np.maximum(0, A)\n",
        "    self.cache[\"A\" + str(layer)] = A\n",
        "    return A\n",
        "  \n",
        "  def _softmax(self, AL):\n",
        "    num_data = AL.shape[1]\n",
        "    result = np.zeros(AL.shape)\n",
        "    # matrix of 1s\n",
        "    for n in range(num_data):\n",
        "      curr = AL[:,n]\n",
        "      e_curr = np.exp(curr - np.max(curr))\n",
        "      result[:,n] = e_curr/np.sum(e_curr)\n",
        "    return result\n",
        "  \n",
        "  def costFunction(self, AL, y):\n",
        "    AL_softmax = self._softmax(AL)\n",
        "    # calculate cross entropy loss\n",
        "    n = len(y)\n",
        "    log_likelyhood = -np.log(AL_softmax[y, range(n)]++1e-9)\n",
        "    loss = np.sum(log_likelyhood) / n\n",
        "    return loss\n",
        "  \n",
        "  \n",
        "  def affineBackward(self, dL_dZ, layer, rate):\n",
        "    A = self.cache[\"A\" + str(layer - 1)]\n",
        "    dL_dW = np.dot(dL_dZ ,A.transpose())\n",
        "    W = self.parameters[\"W\" + str(layer)]\n",
        "    self.parameters[\"W\" + str(layer)] =  self.parameters[\"W\" + str(layer)] - rate * dL_dW\n",
        "    self.parameters[\"b\" + str(layer)] =  self.parameters[\"b\" + str(layer)] - rate * np.dot(dL_dZ, np.ones((dL_dZ.shape[1],1)))\n",
        "    return np.dot(W.transpose(), dL_dZ)\n",
        "     \n",
        "  \n",
        "  \n",
        "  def activationBackward(self, dL_dA, layer):\n",
        "    Z = self.cache[\"Z\" + str(layer - 1)]\n",
        "    Z[Z <= 0] = 0\n",
        "    Z[Z > 1] = 1\n",
        "    # multiplication\n",
        "    return dL_dA*Z\n",
        "    \n",
        "  def backPropagation(self, y):\n",
        "    AL = self.cache[\"Z\" + str(self.num_layers)]\n",
        "    grad = self._softmax(AL)\n",
        "    n = len(y)\n",
        "    grad[y, range(n)] -= 1\n",
        "    grad = grad / n\n",
        "    return grad\n",
        "  \n",
        "  def forward(self, X_train):\n",
        "    self.cache[\"A0\"] = X_train\n",
        "    for layer in range(1, self.num_layers):\n",
        "      Z = self.affineForward(self.cache[\"A\"+ str(layer - 1)], self.parameters[\"W\" + str(layer)], self.parameters[\"b\" + str(layer)], layer)\n",
        "      A = self.activationForward(Z, layer)\n",
        "    Z = self.affineForward(self.cache[\"A\" + str(self.num_layers - 1)], self.parameters[\"W\" + str(self.num_layers)], self.parameters[\"b\" + str(self.num_layers)], self.num_layers)\n",
        "    return Z\n",
        "  \n",
        "  def predict(self, X_test):\n",
        "    Z = self.forward(X_test)\n",
        "    Z = self._softmax(Z)\n",
        "    Y_Pred = np.argmax(Z, axis=0)\n",
        "    return Y_Pred\n",
        "\n",
        "  def validate(self, valiset_X, valiset_Y):\n",
        "    Y_pred = self.predict(valiset_X)\n",
        "    correct = 0\n",
        "    for i in range(0, len(Y_pred)):\n",
        "      if (Y_pred[i] == valiset_Y[i]):\n",
        "        correct += 1\n",
        "    return correct / len(valiset_Y)\n",
        "  \n",
        "  def train(self, trainset, X_val, Y_val, iters, alpha, batch_size):\n",
        "    \n",
        "    for iter in range(iters):\n",
        "      print(\"Iteration: \" + str(iter))\n",
        "      random.shuffle(trainset)\n",
        "      X_train = [item[0].tolist() for item in trainset]\n",
        "      Y_train = [item[1] for item in trainset]\n",
        "      l = 0\n",
        "      for i in range(0, len(X_train)-1, batch_size):\n",
        "        X_train_mini = np.transpose(X_train[i : i + batch_size])\n",
        "        Y_train_mini = np.transpose(Y_train[i : i + batch_size])\n",
        "        Z = self.forward(X_train_mini)\n",
        "        loss = self.costFunction(Z,Y_train_mini)\n",
        "        dL_dZ = self.backPropagation(Y_train_mini)\n",
        "        for layer in range(self.num_layers, 0, -1):\n",
        "          dL_dA = self.affineBackward(dL_dZ,layer, alpha)\n",
        "          if layer > 1:\n",
        "            dL_dZ = self.activationBackward(dL_dA, layer)\n",
        "        l = loss\n",
        "      train_accuracy = self.validate(trainset_X.transpose(), trainset_Y)\n",
        "      validate_accuracy = self.validate(valiset_X.transpose(), valiset_Y)\n",
        "      \n",
        "        \n",
        "      print(\"training loss is \", l)\n",
        "      print(\"training accuracy is \", train_accuracy)\n",
        "      print(\"validation accuracy is \", validate_accuracy)\n",
        "      print(\" \")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  layer_dimensions = [3072, 600, 500, 10]\n",
        "  nn = NeuralNetwork(layer_dimensions)\n",
        "  nn.train(training_set, [], [], 15, 0.01, 50)\n",
        " \n",
        "  "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 0\n",
            "training loss is  1.7764639465987546\n",
            "training accuracy is  0.40275555555555553\n",
            "validation accuracy is  0.3888\n",
            " \n",
            "Iteration: 1\n",
            "training loss is  1.747398341337623\n",
            "training accuracy is  0.4446\n",
            "validation accuracy is  0.4332\n",
            " \n",
            "Iteration: 2\n",
            "training loss is  1.6867776195266115\n",
            "training accuracy is  0.4696\n",
            "validation accuracy is  0.4464\n",
            " \n",
            "Iteration: 3\n",
            "training loss is  1.3833475200976517\n",
            "training accuracy is  0.4975777777777778\n",
            "validation accuracy is  0.467\n",
            " \n",
            "Iteration: 4\n",
            "training loss is  1.3868560710314843\n",
            "training accuracy is  0.5142222222222222\n",
            "validation accuracy is  0.4802\n",
            " \n",
            "Iteration: 5\n",
            "training loss is  1.3291142033425338\n",
            "training accuracy is  0.5268666666666667\n",
            "validation accuracy is  0.4924\n",
            " \n",
            "Iteration: 6\n",
            "training loss is  1.281938767161841\n",
            "training accuracy is  0.5373555555555556\n",
            "validation accuracy is  0.4852\n",
            " \n",
            "Iteration: 7\n",
            "training loss is  1.500319052193226\n",
            "training accuracy is  0.5626666666666666\n",
            "validation accuracy is  0.5008\n",
            " \n",
            "Iteration: 8\n",
            "training loss is  1.1904887867307246\n",
            "training accuracy is  0.5728888888888889\n",
            "validation accuracy is  0.5064\n",
            " \n",
            "Iteration: 9\n",
            "training loss is  1.5766854566462172\n",
            "training accuracy is  0.5815555555555556\n",
            "validation accuracy is  0.5118\n",
            " \n",
            "Iteration: 10\n",
            "training loss is  1.360174404643718\n",
            "training accuracy is  0.6003111111111111\n",
            "validation accuracy is  0.5176\n",
            " \n",
            "Iteration: 11\n",
            "training loss is  1.279970640307238\n",
            "training accuracy is  0.6043111111111111\n",
            "validation accuracy is  0.5174\n",
            " \n",
            "Iteration: 12\n",
            "training loss is  1.1442031490033984\n",
            "training accuracy is  0.629\n",
            "validation accuracy is  0.529\n",
            " \n",
            "Iteration: 13\n",
            "training loss is  1.1652994774786445\n",
            "training accuracy is  0.6316\n",
            "validation accuracy is  0.5292\n",
            " \n",
            "Iteration: 14\n",
            "training loss is  0.9718383981359372\n",
            "training accuracy is  0.6466444444444445\n",
            "validation accuracy is  0.531\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dn9JRbhSgviw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}