{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhuchunlin1995/Deep-Learning/blob/master/image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "6SD-AONjrguR",
        "colab_type": "code",
        "outputId": "0430ed0d-3b85-4f39-8130-247922b41a24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "#download the CIFAR10 dataset for image classification.\n",
        "import torch, torchvision\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "transform = torchvision.transforms.Compose( [torchvision.transforms.ToTensor(),torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "\n",
        "#preprocess trainset.\n",
        "dataset = []\n",
        "#flat dataset\n",
        "for data in trainset:\n",
        "  tmp = []\n",
        "  tmp.append(data[0].numpy().ravel())\n",
        "  tmp.append(data[1])\n",
        "  dataset.append(tmp)\n",
        "\n",
        "\n",
        "random.shuffle(dataset)\n",
        "#separate features and label\n",
        "dataset_X = [item[0].tolist() for item in dataset]\n",
        "dataset_Y = [item[1] for item in dataset]\n",
        "\n",
        "#split dataset into training set and validation set and format the dataset\n",
        "\n",
        "valiset_X = np.asarray(dataset_X[:5000])\n",
        "valiset_Y = dataset_Y[:5000]\n",
        "\n",
        "\n",
        "trainset_X = np.asarray(dataset_X[5000:]) #shape (num of training data, num of features)\n",
        "trainset_Y = dataset_Y[5000:]\n",
        "\n",
        "\n",
        "print(len(trainset_Y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "45000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EtTKFOi5g1IJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8abe39e5-ae5e-4b06-8585-b3f61ab45bf1"
      },
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  \n",
        "  \"\"\"we will initialize the number of neuron for every hidden layer as 2/3 size of the input layer,\n",
        "    and assume the output layer will only contains only one neuron\"\"\"\n",
        "  def __init__(self, layer_dimensions):\n",
        "    \n",
        "    #initialize weights and bias\n",
        "    self.parameters = {}\n",
        "    self.num_layers = len(layer_dimensions)\n",
        "    for l in range (1, self.num_layers):\n",
        "      eps = np.sqrt(2.0 / (layer_dimensions[l] + layer_dimensions[l - 1]))\n",
        "      self.parameters[\"W\" + str(l)] = np.random.randn(layer_dimensions[l - 1], layer_dimensions[l]) * eps #shape (features, neurons)\n",
        "      self.parameters[\"b\" + str(l)] = np.zeros((1, layer_dimensions[l])) + 0.01 #shape (1,n)\n",
        "   \n",
        "  def affineFoward(self, A, W, b):\n",
        "    forward = np.dot(A, W) + b\n",
        "    return forward\n",
        "    \n",
        "  def activationForward(self, A):\n",
        "    # use relu function as activation function\n",
        "    A = np.maximum(0, A)\n",
        "    return A\n",
        "  \n",
        "  def _softmax(AL):\n",
        "    return np.exp(AL)/sum(np.exp(AL), axis=0)\n",
        "  \n",
        "  def costFunction(self, AL, y):\n",
        "    # use softmax function to normalize AL first\n",
        "    AL_softmax = _softmax(AL)\n",
        "    # calculate cross entropy loss\n",
        "    n = len(y)\n",
        "    log_likelyhood = -np.log(AL_softmax[range(m), y])\n",
        "    loss = np.sum(log_likelyhood) / m\n",
        "    return loss\n",
        "      \n",
        "  \n",
        "  def affineBackward(self, dA_prev, cache):\n",
        "    \n",
        "  \n",
        "  \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  \"\"\"we will initialize the number of neuron for every hidden layer as 2/3 size of the input layer,\n",
        "    and assume the output layer will only contains only one neuron\"\"\"\n",
        "  \n",
        "  layer_dimensions = [3072, 2048, 1365, 1]\n",
        "  nn = NeuralNetwork(layer_dimensions)\n",
        "  \n",
        "  print(nn.parameters[\"b1\"].shape)\n",
        "      \n",
        "    "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2048, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a0KK-hWuopfg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a4048c4a-726f-4b9f-ba8e-5cb3960ec0b4"
      },
      "cell_type": "code",
      "source": [
        "A = np.random.randn(5, 10)\n",
        "y = [1,2,3,4,5]\n",
        "print(A)\n",
        "A[range(5), y]\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.55372208 -0.6272012  -1.41055966 -1.62713287 -0.21855507  0.52050926\n",
            "  -0.30800945  0.36211905 -1.81514557 -1.04005655]\n",
            " [-0.16991465  0.56382207 -0.12679905  0.49046383  0.65722352 -0.35200741\n",
            "  -0.49296342  0.74670448  1.45856258 -0.66630553]\n",
            " [-0.49386802  0.33298479 -0.43847143 -0.71420226  1.01764132  0.53350237\n",
            "  -0.28806251 -0.83567213 -0.41701604  0.16851141]\n",
            " [-0.08486893  0.3566247  -0.16882935  0.77027789 -0.1135793   0.33548316\n",
            "  -2.39281474 -1.05559077 -1.23997733 -0.83135663]\n",
            " [ 1.53647943  0.10074289 -0.01387119  0.57493316  0.15178305 -2.00119277\n",
            "  -0.18622262 -0.04394314  0.19359048 -0.68596562]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.6272012 , -0.12679905, -0.71420226, -0.1135793 , -2.00119277])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "dn9JRbhSgviw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}